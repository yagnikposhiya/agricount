{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bAsDDpUVoIx"
      },
      "source": [
        "author: Yagik Poshiya\n",
        "github: @yagnikposhiya\n",
        "organization: Tvisi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WMkDff2Vuvh",
        "outputId": "2005a4d6-a194-4f5d-df32-ac0a714ebe02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive to the current session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "G6beGdWkWKpP",
        "outputId": "ce68c54c-b1c2-4e5a-b0ac-5442ab58471f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd() # check which one is the current working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BSoiFmdWYX4",
        "outputId": "c04358dd-f68a-4a0a-fbd1-c4d2c4ffd1b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data', 'train.ipynb']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir('/content/drive/MyDrive/Notebook_Testing') # list all directories or files available in the specified path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAwr3hr4XVPV",
        "outputId": "ee7cebcd-9311-4bfc-aa07-3bbcca88e5ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.12.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.12.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.12.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.5\n"
          ]
        }
      ],
      "source": [
        "# install weights-and-biases using pip\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Pfb---XbWf",
        "outputId": "28cbcb52-de63-4444-d421-be0f239e9514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.3.3-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.26.4)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
            "Requirement already satisfied: torch<4.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.3.1+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=2.0.0->lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.0.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.0.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n",
            "Downloading lightning-2.3.3-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.5/808.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.3.3 lightning-utilities-0.11.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.3.3 torchmetrics-1.4.0.post0\n"
          ]
        }
      ],
      "source": [
        "# install pytorch_lightning using pip\n",
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_Hop4smX5sK",
        "outputId": "2313a5cf-74d3-4c73-97c2-99efb0d5f89f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "# install torchinfo for printing model summary using pip\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tdm1-r9VoIz"
      },
      "source": [
        "### Import required python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "37rTeVxVVoI0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import json\n",
        "import wandb\n",
        "import torch\n",
        "import shutil\n",
        "import random\n",
        "import subprocess\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from typing import Any\n",
        "from torchinfo import summary\n",
        "from collections import Counter\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnV3MAaYVoI1"
      },
      "source": [
        "### Define required functions & classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cV67NNbvVoI2"
      },
      "outputs": [],
      "source": [
        "def check_gpu_config() -> None:\n",
        "    \"\"\"\n",
        "    This function is used to check, whether GPUs are available.\n",
        "\n",
        "    Parameters:\n",
        "    - (None)\n",
        "\n",
        "    Returns:\n",
        "    - (None)\n",
        "    \"\"\"\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        num_gpus = torch.cuda.device_count() # get total number of available gpus\n",
        "        print(\"- Number of GPUs available: {}\".format(num_gpus))\n",
        "\n",
        "        for i in range(num_gpus):\n",
        "            gpu_name = torch.cuda.get_device_name() # get gpu name\n",
        "            print(\"- GPU name: {}\".format(gpu_name))\n",
        "\n",
        "        command = \"nvidia-smi\" # set a command\n",
        "        result = subprocess.run(command, shell=True, capture_output=True, text=True) # execute a command\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(result.stdout) # output after successful execution of command\n",
        "        else:\n",
        "            print(\"- Error message: \\n{}\".format(result.stderr)) # output after failed execution of command\n",
        "\n",
        "    else:\n",
        "        print(\"- CUDA is not available. Using CPU instead.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KzQZWw0hVoI2"
      },
      "outputs": [],
      "source": [
        "class Config():\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        # current working directory\n",
        "        # self.CWD = os.getcwd() # get current working directory\n",
        "        self.CWD = '/content/drive/MyDrive/Notebook_Testing/'\n",
        "\n",
        "        # training and validation set paths\n",
        "        self.TRAINSET_PATH = '' # set training set path\n",
        "        self.VALIDSET_PATH = '' # set validation set path\n",
        "        self.TRAIN_IMAGE_DIR = '' # set train/images directory path\n",
        "        self.TRAIN_MASK_DIR = '' # set train/masks directory path\n",
        "\n",
        "        # other relevant paths\n",
        "        self.INPUT_JSON_FILE_PATH = os.path.join(self.CWD,'data/raw/json_projects') # set path to the directory which contains one or more than one json files\n",
        "        self.SAMPLE_JSON_FILE_PATH = os.path.join(self.CWD,'data/raw/json_projects/File1.json')  # set path to the json file for understanding json file structure\n",
        "        self.RAW_IMAGE_DIR = os.path.join(self.CWD,'data/raw/images') # set raw image directory\n",
        "        self.BASE_DATA_PATH = os.path.join(self.CWD,'data') # set base data folder path\n",
        "        self.PATH_TO_SAVE_TRAINED_MODEL = os.path.join(self.dir,'saved_models') # set path to save trained model\n",
        "\n",
        "        # weight and biases config\n",
        "        self.ENTITY = 'neuralninjas' # set team/organization name for wandb account\n",
        "        self.PROJECT = 'tvisi-agri-count' # set project name\n",
        "        self.REINIT = True # set boolean value for reinitialization\n",
        "        self.ANONYMOUS = 'allow' # set anonymous value type\n",
        "        self.LOG_MODEL = 'all' # set log model type\n",
        "\n",
        "        # model training parameters\n",
        "        self.BATCH_SIZE = 16 # set batch size for model training\n",
        "        self.MAX_EPOCHS = 2 # set maximum epochs for model training\n",
        "        self.NUM_CLASSES = 2 # set number of classes contains by mask images (in segmentation case)\n",
        "        self.LEARNING_RATE = 0.001 # set learning rate\n",
        "        self.TRANSFORM = True # set booelan values for applying augmentation techniques for training set\n",
        "\n",
        "\n",
        "    def printConfiguration(self) -> None:\n",
        "        \"\"\"\n",
        "        This function is used to print all configuration related to paths and model training params\n",
        "\n",
        "        Parameters:\n",
        "        - (None)\n",
        "\n",
        "        Returns:\n",
        "        - (None)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"-----------------------------------------------------------\")\n",
        "        print(\"-----------------------CONFIGURATIONS----------------------\")\n",
        "        print(\"-----------------------------------------------------------\")\n",
        "        print(\"\\n\",\n",
        "              f\"- Current working directory: {self.CWD}\\n\",\n",
        "              f\"- Trainset path: {self.TRAINSET_PATH}\\n\",\n",
        "              f\"- Validset path: {self.VALIDSET_PATH}\\n\",\n",
        "              f\"- Train image directory: {self.TRAIN_IMAGE_DIR}\\n\",\n",
        "              f\"- Train mask directory: {self.TRAIN_MASK_DIR}\\n\",\n",
        "              f\"- Input JSON file path: {self.INPUT_JSON_FILE_PATH}\\n\",\n",
        "              f\"- Sample JSON file path: {self.SAMPLE_JSON_FILE_PATH}\\n\",\n",
        "              f\"- Raw image directory: {self.RAW_IMAGE_DIR}\\n\",\n",
        "              f\"- Base data path: {self.BASE_DATA_PATH}\\n\",\n",
        "              f\"- Path to save trained model: {self.PATH_TO_SAVE_TRAINED_MODEL}\\n\",\n",
        "              f\"- Batch size: {self.BATCH_SIZE}\\n\",\n",
        "              f\"- Maximum epochs: {self.MAX_EPOCHS}\\n\",\n",
        "              f\"- Number of classes: {self.NUM_CLASSES}\\n\",\n",
        "              f\"- Learning rate: {self.LEARNING_RATE}\\n\",\n",
        "              f\"- Tranformation/Augmentation: {self.TRANSFORM}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Awi6CKjgVoI3"
      },
      "outputs": [],
      "source": [
        "class FileDoesNotExist(BaseException): # create custom class to raise an error\n",
        "    pass # no actions are needed\n",
        "\n",
        "class DirectoryDoesNotExist(BaseException): # create custom class to raise an error\n",
        "    pass # no actions are needed\n",
        "\n",
        "def look_at_json_structure(json_file_path: str) -> None:\n",
        "    \"\"\"\n",
        "    This function is used to understand the JSON file structure and what information it contains.\n",
        "\n",
        "    Parameters:\n",
        "    - json_file_path (str): Input json file path which contains mask region XY co-ordinates\n",
        "\n",
        "    Returns:\n",
        "    - (None)\n",
        "    \"\"\"\n",
        "\n",
        "    input_file = open(json_file_path, 'r') # open json file in read-only mode\n",
        "    input_data = json.load(input_file) # laod json data from input_file\n",
        "\n",
        "    # print(input_data) # print json data in output console\n",
        "    # print(type(input_data)) # <class 'dict'>\n",
        "\n",
        "    print(\"- Keys available in json data: \\n{}\\n\".format(input_data.keys())) # all major keys available in the json data\n",
        "    # print(\"- Values of keys available in json data: \\n{}\".format(input_data.values())) # all major values available in the json data\n",
        "\n",
        "    \"\"\"\n",
        "    Major keys available in json file:\n",
        "    dict_keys(['_via_settings', '_via_img_metadata', '_via_attributes', '_via_data_format_version', '_via_image_id_list'])\n",
        "\n",
        "    Out of these keys '_via_img_metadata' key contains mask region information.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"- Keys in value of '_via_img_metadata' key: \\n{}\\n\".format(input_data['_via_img_metadata'].keys())) # keys in value of '_via_img_metadata' key\n",
        "    # as an output of above line, we'll get name of the images for those mask regions are there.\n",
        "\n",
        "    print(\"- Keys in value of '9.jpeg136245' key: \\n{}\\n\".format(input_data['_via_img_metadata']['9.jpeg136245'].keys())) # keys in value of '9.jpeg136245' key\n",
        "    # dict_keys(['filename', 'size', 'regions', 'file_attributes'])\n",
        "\n",
        "    print(\"- Filename: {}\".format(input_data['_via_img_metadata']['9.jpeg136245']['filename'])) # filename\n",
        "    # print(\"- Regions: {}\".format(type(input_data['_via_img_metadata']['9.jpeg136245']['regions']))) # <class 'list'>\n",
        "    print(\"- Regions: {}\".format(len(input_data['_via_img_metadata']['9.jpeg136245']['regions']))) # total of regions available in an image\n",
        "    print(\"- First region information:\")\n",
        "    print(\"-- Class name: {}\".format(input_data['_via_img_metadata']['9.jpeg136245']['regions'][0]['region_attributes']['name'].rstrip('\\n'))) # extract class name or extract main part of a string; do not want last \\n character\n",
        "    print(\"-- X co-ordinates: \\n{}\".format(input_data['_via_img_metadata']['9.jpeg136245']['regions'][0]['shape_attributes']['all_points_x'])) # X co-ordinates\n",
        "    print(\"-- Y co-ordinates: \\n{}\\n\".format(input_data['_via_img_metadata']['9.jpeg136245']['regions'][0]['shape_attributes']['all_points_y'])) # Y co-ordinates\n",
        "\n",
        "def createMasks(json_file_path: str, raw_image_dir:str, base_data_path:str) -> str:\n",
        "    \"\"\"\n",
        "    This function is used to create mask image from the existing information available in the\n",
        "    json file related to mask regions in each image.\n",
        "\n",
        "    Parameters:\n",
        "    - json_file_path (str): Input json file path which contains mask region XY co-ordinates\n",
        "    - raw_image_dir (str): Directory path contains raw images mentioned in the json file\n",
        "    - base_data_path (str): Path for data directory\n",
        "\n",
        "    Returns:\n",
        "    - (None)\n",
        "    \"\"\"\n",
        "\n",
        "    input_file = open(json_file_path, 'r') # open json file in read-only mode\n",
        "    input_data = json.load(input_file) # laod json data from input_fi\n",
        "\n",
        "    if not (os.path.exists(raw_image_dir)): # check if images directory exist or not\n",
        "        raise DirectoryDoesNotExist(raw_image_dir) # raise an error directory does not exist\n",
        "    elif not (os.path.exists(f'{base_data_path}/processed')): # check if masks directory exists or not\n",
        "        os.makedirs(f'{base_data_path}/processed') # if not then create it\n",
        "\n",
        "    if not (os.path.exists(f'{base_data_path}/processed/train')): # check if train directory exists or not\n",
        "        os.makedirs(f'{base_data_path}/processed/train') # if not then create it\n",
        "\n",
        "        if not (os.path.exists(f'{base_data_path}/processed/train/images')): # check if images directory exists or not\n",
        "            os.makedirs(f'{base_data_path}/processed/train/images') # if not then create it\n",
        "\n",
        "        if not (os.path.exists(f'{base_data_path}/processed/train/masks')): # check if masks directory exists or not\n",
        "            os.makedirs(f'{base_data_path}/processed/train/masks') # if not then create it\n",
        "\n",
        "    trainset_path = f'{base_data_path}/processed/train' # set trainset path\n",
        "    trainset_images_path = f'{base_data_path}/processed/train/images' # set trainset images path\n",
        "    # input_data = input_data['_via_img_metadata'] # extract data related to mask regions only\n",
        "\n",
        "    for filename in os.listdir(raw_image_dir): # list all files available in the images directory of raw directory\n",
        "        full_filename = os.path.join(raw_image_dir,filename) # create full filename\n",
        "        if os.path.exists(full_filename): # check if file available or not\n",
        "            shutil.copy(full_filename,trainset_images_path) # if available then copy that file to destination path\n",
        "        else:\n",
        "            raise FileDoesNotExist(full_filename) # raise an error file does not exist\n",
        "\n",
        "    file_names = [] # create an empty list to store filenames\n",
        "    heights = [] # create an empty list to store image heights\n",
        "    widths = [] # create an empty list to store image widths\n",
        "    channels = [] # create en empty list to store image channels\n",
        "\n",
        "    for key,value in input_data.items():\n",
        "        filename = value ['filename'] # extract filename i.e. 9.jpeg\n",
        "        image_path = f'{trainset_path}/images/{filename}' # set processed image path\n",
        "        mask_path = f'{trainset_path}/masks/{filename}' # set processed mask path\n",
        "\n",
        "        if not (os.path.exists(image_path)): # check whether image exist or not\n",
        "            raise FileDoesNotExist(image_path) # raise an error file does not exist\n",
        "        else:\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_COLOR) # load colored image\n",
        "            h, w, channel = image.shape\n",
        "\n",
        "            heights.append(h) # append height of corresponding image\n",
        "            widths.append(w) # append width of corresponding image\n",
        "            channels.append(channel) # append channel of corresponding image\n",
        "            file_names.append(image_path) # append image path\n",
        "\n",
        "    # finding most common height and width\n",
        "    most_common_height = Counter(heights).most_common(1)[0][0] # SINGLE most common height; returns a list contains tuple (common_height,count); here common_height is favorable then additional [0] index at last\n",
        "    most_common_width = Counter(widths).most_common(1)[0][0] # SINGLE most common width; same explanation as above\n",
        "    most_common_channel = Counter(channels).most_common(1)[0][0] # SINGLE most common channel; same explanation as above\n",
        "\n",
        "    filtered_filenames = [] # define empty list to store filenames\n",
        "\n",
        "    # The zip function is particularly useful when you want to iterate over multiple sequences in parallel/simultaneously.\n",
        "    for filename_local, height, width, channel in zip(file_names, heights, widths, channels): # create\n",
        "        if height == most_common_height and width == most_common_width and channel == most_common_channel:\n",
        "            filtered_filenames.append(filename_local)\n",
        "        else:\n",
        "            os.remove(filename_local) # if file has different shape than common shape then remove file from processed directory\n",
        "            print(f'File has been removed from processed dataset.: {filename_local}')\n",
        "\n",
        "    for key, value in input_data.items():\n",
        "        fname = value['filename'] # extract filename\n",
        "        image_path = f'{trainset_path}/images/{fname}' # create full image path\n",
        "\n",
        "        if image_path in filtered_filenames: # if image_path is available in the filtered_filenames\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_COLOR) # load colored image\n",
        "            h, w, _ = image.shape # extract height and width of the image\n",
        "\n",
        "            mask = np.zeros((h,w)) # create zero-array of size (height,width) for mask\n",
        "\n",
        "            regions = value['regions'] # extract mask regions related information to the loaded image\n",
        "\n",
        "            for region in regions:\n",
        "                shape_attributes = region['shape_attributes'] # extract information regarding shape attributes\n",
        "                x_points = shape_attributes['all_points_x'] # extract all x co-ordinates\n",
        "                y_points = shape_attributes['all_points_y'] # extract all y co-ordinates\n",
        "\n",
        "                contours = [] # define an empty list to store contours\n",
        "\n",
        "                for x, y in zip(x_points,y_points):\n",
        "                    contours.append((x,y)) # append tuple of x-y co-ordinates to the contours list\n",
        "\n",
        "                contours = np.array(contours) # convert contours from list to numpy array\n",
        "\n",
        "                cv2.drawContours(mask, [contours], -1, 255, -1) # draw contours in an image\n",
        "\n",
        "            # apply morphological operations\n",
        "            kernel = np.ones((3,3), np.uint8) # define kernel for morphological operation\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2) # used to separate touching objects\n",
        "\n",
        "            mask = np.uint8(mask/255) # label the pixel; either 0 (background) or 1 (Peanut Seed)\n",
        "\n",
        "            mask_path = f'{trainset_path}/masks/{fname}' # set path for saving a masks\n",
        "            cv2.imwrite(mask_path, mask) # save mask to the specified path with the same filename as image has to local storage\n",
        "\n",
        "        else:\n",
        "            print(f'File is not available in the dataset.: {image_path}') # if file(image_path) is not available in the filtered_filenames list\n",
        "\n",
        "    print(\"Mask image are created successfully.\")\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir:str, masks_dir:str, transform:bool=False) -> None:\n",
        "        self.images_dir = images_dir # set path to directory contains input images\n",
        "        self.masks_dir = masks_dir # set path to directory contains mask images\n",
        "        self.transform = transform # set boolean value for transform/augmentation\n",
        "\n",
        "        self.image_files = sorted(os.listdir(images_dir)) # generate a list of input images\n",
        "        self.mask_files = sorted(os.listdir(masks_dir)) # generate a list of mask images\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_files) # return total number of data samples available in the dataset\n",
        "\n",
        "    def __getitem__(self,index) -> Any:\n",
        "        image_path = os.path.join(self.images_dir, self.image_files[index]) # generate path for a single image\n",
        "        mask_path = os.path.join(self.masks_dir, self.mask_files[index]) # generate path for a single mask image\n",
        "\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_COLOR) # read an input image as a colored image\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) # read a mask image as a grayscale image\n",
        "\n",
        "        image = np.transpose(image, (2, 0, 1)) # permute shape from (720,1280,3) to (3,720,1280)\n",
        "\n",
        "        \"\"\"\n",
        "        There is no need to do anything related to one-hot encoding for mask images. Once you have clarify the number of classes\n",
        "        at the time of the model implementation then it will work in the segmentation task.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Tensors with negative strides are not currently supported that is why we have to use PIL and\n",
        "        then have to apply transformations.\n",
        "        \"\"\"\n",
        "\n",
        "        # image = Image.fromarray(image).convert(\"L\") # convert to PIL grayscale images\n",
        "        # mask = Image.fromarray(mask).convert(\"L\") # convert to PIL grayscale images\n",
        "\n",
        "        # if self.transform: # if transform/augmentation value is set to True\n",
        "        #     random_int = random.randint(0,2) # return random interge including both end-points\n",
        "        #     if random_int == 0:\n",
        "        #         image = image # no transformation/augmentation technique is applied\n",
        "        #     elif random_int == 1:\n",
        "        #         image = np.flip(image, axis=0) # i.e. vertical flipping (first raw & last raw) -> (last raw -> first raw)\n",
        "        #         mask = np.flip(mask, axis=0) # i.e. vertical flipping\n",
        "        #     elif random_int == 2:\n",
        "        #         image = np.flip(image, axis=1) # i.e. horizontal flipping (first column & last column) -> (last column -> first column)\n",
        "        #         mask = np.flip(mask, axis=1) # i.e. horizontal flipping\n",
        "\n",
        "        # image = image.unsqueeze(0) # add a channel dimension\n",
        "\n",
        "        # # convert back to numpy arrays\n",
        "        # image = np.array(image)\n",
        "        # mask = np.array(mask)\n",
        "\n",
        "        # # remove negative strides\n",
        "        # image = image.copy()\n",
        "        # mask = mask.copy()\n",
        "\n",
        "        image = torch.tensor(image, dtype=torch.float32) # convert to torch tensor\n",
        "        mask = torch.tensor(mask, dtype=torch.long) # convert to torch tensor\n",
        "\n",
        "        return image, mask # return image and mask in the form of tuple\n",
        "\n",
        "class SegmentationDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_image_dir:str, train_mask_dir:str, batch_size:int=32, transform:bool=False, val_split:float=0.2, test_split:float=0.1) -> None:\n",
        "        super(SegmentationDataModule, self).__init__()\n",
        "        self.train_image_dir = train_image_dir # set path to directory contains input images for training\n",
        "        self.train_mask_dir = train_mask_dir # set path to directory contains input mask images for training\n",
        "        self.batch_size = batch_size # set batch size\n",
        "        self.transform = transform # set boolean value for transformation/augmentation\n",
        "        self.val_split = val_split # set validation set split ratio\n",
        "        self.test_split = test_split # set test set split ratio\n",
        "\n",
        "    def setup(self, stage=None) -> None:\n",
        "        self.train_dataset = SegmentationDataset(self.train_image_dir, self.train_mask_dir, self.transform) # create an instance of SegmentationDataset class and load data samples as needed\n",
        "        # self.val_dataset = SegmentationDataset(self.val_image_dir, self.val_mask_dir) # same as trainset loading just transform is not applied to the validation set\n",
        "        val_size = int(len(self.train_dataset) * self.val_split) # calculate validation set size\n",
        "        # test_size = int(len(self.train_dataset) * test_size) # calculate test set size\n",
        "        # train_size = len(self.train_dataset) - val_size - test_size # calculate train set size\n",
        "        train_size = len(self.train_dataset) - val_size # calculate train set size\n",
        "\n",
        "        # self.train_dataset, self.val_dataset, self.test_dataset = random_split(self.train_dataset, [train_size,val_size, test_size]) # split whole dataset into train and validation\n",
        "        self.train_dataset, self.val_dataset = random_split(self.train_dataset, [train_size,val_size]) # split whole dataset into train and validation\n",
        "\n",
        "    def train_dataloader(self) -> Any:\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) # return train set\n",
        "\n",
        "    def val_dataloader(self) -> Any:\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size) # return validation set\n",
        "\n",
        "    # def test_dataloader(self) -> Any:\n",
        "    #     return DataLoader(self.test_dataset, batch_size=self.batch_size) # return test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q4-AgnnNVoI4"
      },
      "outputs": [],
      "source": [
        "# matplotlib.use('TkAgg') # or 'Qt5Agg' or any other backend that supports interactive display\n",
        "# # by default backend: FigureCanvasAgg\n",
        "\n",
        "class DirectoryDoesNotExist(BaseException): # define a custom class for exception and raise if directory does not exist/available\n",
        "    pass\n",
        "\n",
        "def merge_json_files(input_files:list, output_file:str) -> Any:\n",
        "    \"\"\"\n",
        "    This function is used to merge json files which have same file structure.\n",
        "\n",
        "    Parameters:\n",
        "    - input_files (list): List of JSON files\n",
        "    - output_file (str): Path to the JSON where merged JSON data will be stored\n",
        "\n",
        "    Returns:\n",
        "    - (None)\n",
        "    \"\"\"\n",
        "\n",
        "    if len(input_files) == 1:\n",
        "        print(f'Only {len(input_files)} json file is detected.')\n",
        "    else:\n",
        "        print(f'Merging {len(input_files)} json files.')\n",
        "\n",
        "    dictionaries_list = [] # define empty dictionaries list\n",
        "\n",
        "    for file in input_files:\n",
        "        input_file = open(file, 'r') # open json file in read-only mode\n",
        "        input_data = json.load(input_file) # load json data from input_file\n",
        "        input_data = input_data['_via_img_metadata'] # extract values of _via_img_metadata key\n",
        "        dictionaries_list.append(input_data) # append extracted values to the list of dictionaries\n",
        "\n",
        "    merged_data = {key: value for d in dictionaries_list for key, value in d.items()} # merging all dictionaries data\n",
        "\n",
        "    directory_path, filename = os.path.split(output_file) # split output_file path into two parts; 1) directory path and 2) filename\n",
        "\n",
        "    if os.path.exists(directory_path): # check that directory exists or not\n",
        "        outfile = open(output_file, 'w') # open json file in writing mode\n",
        "        json.dump(merged_data, outfile) # write merged json data to the output_file with indent=4\n",
        "    else:\n",
        "        raise DirectoryDoesNotExist(directory_path) # raise an exception that directory does not exist\n",
        "\n",
        "    if len(input_files) > 1: # if more than one json files are available\n",
        "        print(f'{len(input_files)} json files are merged successfully.')\n",
        "\n",
        "\n",
        "def show_mask_image_pair(image_dir:str, mask_dir:str) -> None:\n",
        "    \"\"\"\n",
        "    This function is used to visualize image-mask pair.\n",
        "\n",
        "    Parameters:\n",
        "    - image_dir (str): Path to image directory in processed data directory\n",
        "    - mask_dir (str): Path to mask directory in processed data directory\n",
        "\n",
        "    Returns:\n",
        "    - (None)\n",
        "    \"\"\"\n",
        "\n",
        "    image_list = glob.glob(f'{image_dir}/*.jpeg') # create list contains all images\n",
        "    mask_list = glob.glob(f'{mask_dir}/*.jpeg') # create list contains all masks\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_choice = str(input('Do you want to visualize mask-image pairs? [Y/N]: ')) # ask user for his/her binary choice\n",
        "            if user_choice.lower() == 'y': # if user enter Y/y\n",
        "                random_num = random.randint(0,len(image_list)-1) # generate random number between 0 and len(image_list)-1\n",
        "                image = cv2.imread(image_list[random_num], cv2.IMREAD_COLOR) # read an image\n",
        "                mask = cv2.imread(mask_list[random_num], cv2.IMREAD_GRAYSCALE) # read a mask\n",
        "\n",
        "                _, image_name = os.path.split(image_list[random_num]) # extract image name from the path\n",
        "                _, mask_name = os.path.split(mask_list[random_num]) # extract mask name from the path\n",
        "\n",
        "                # opacity = 0.5 # set desired opacity for the mask image\n",
        "                # mask_color = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR) # convert mask to a 3-channel image\n",
        "\n",
        "                # # apply a color to a mask for better visualization\n",
        "                # mask_color[:, :, 1] = 0 # zero out the green channel\n",
        "                # mask_color[:, :, 2] = 255 # max out the red channel\n",
        "\n",
        "                # overlay_image = cv2.addWeighted(image, 1-opacity, mask_color, opacity, 0) # overlay the mask onto original image\n",
        "\n",
        "                fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(40,40)) # create a canvas with 1 rows and 2 cols with 20*20 figure size\n",
        "\n",
        "                ax1.imshow(image, cmap='gray') # visualize an image\n",
        "                ax1.set_title(f'{image_name}') # set image name as a title\n",
        "                ax1.axis('off') # do not visualize an image with an axis\n",
        "\n",
        "                ax2.imshow(mask, cmap='gray') # visualize a mask\n",
        "                ax2.set_title(f'{mask_name}') # set mask name as a title\n",
        "                ax2.axis('off') # do not visualize an image with an axis\n",
        "\n",
        "                # ax3.imshow(cv2.cvtColor(overlay_image, cv2.COLOR_BGR2RGB)) # visualize an overlay image\n",
        "                # ax3.set_title('Overlay image') # set title for figure\n",
        "                # ax3.axis('off') # do not visualize overlay image with an axis\n",
        "\n",
        "                plt.show() # display all plots/graphs\n",
        "\n",
        "            elif user_choice.lower() == 'n': # if user enters either N or n\n",
        "                return None # stop loop execution\n",
        "\n",
        "            else:\n",
        "                print('Enter \\'Y\\' or \\'N\\'') # ask user to enter either 'Y' or 'N'\n",
        "\n",
        "        except BaseException:\n",
        "            pass # do nothing ignore the exception\n",
        "\n",
        "def get_user_choice(start:int, end:int) -> int:\n",
        "    \"\"\"\n",
        "    This function is used to get integer user choice within specific range including both end-points.\n",
        "\n",
        "    Parameters:\n",
        "    - start (int): Starting point of the range\n",
        "    - end (int: Ending point of the range\n",
        "\n",
        "    Returns:\n",
        "    - (int): Integer user choice\n",
        "    \"\"\"\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_choice = int(input(f'Enter an integer number between {start} and {end}: ')) # ask user to enter his/her choice\n",
        "\n",
        "            if start <= user_choice <= end:\n",
        "                return user_choice\n",
        "            else:\n",
        "                print(f'Invalid number. Enter an integer number between {start} and {end}') # ask user to enter a choice between specified range\n",
        "        except ValueError:\n",
        "            print(f'Invalid number. Enter an integer number between {start} and {end}') # ask user to enter a choice between specified range\n",
        "\n",
        "def available_models() -> tuple:\n",
        "    \"\"\"\n",
        "    This is used to provide a list of neural net architectures available for training on the existing dataset(s).\n",
        "\n",
        "    Parameters:\n",
        "    - (None)\n",
        "\n",
        "    Returns:\n",
        "    - (list,int): Returns tuple contains list of available neural net archs and user choice; (available_nn_arch, user_choice)\n",
        "    \"\"\"\n",
        "\n",
        "    models = ['DNA-Segment'] # list of available models\n",
        "\n",
        "    print('Select any one neural net architecture from the list given below')\n",
        "    for i in range(len(models)):\n",
        "        print(f'{i}_________{models[i]}') # print list of available models with the integer model number\n",
        "\n",
        "    if (len(models)-1) == 0:\n",
        "        return models, np.uint8(0) # only one neural net architecture is there; no need to ask to user for their choice\n",
        "    else:\n",
        "        return models, get_user_choice(0,len(models)-1) # get user choice\n",
        "\n",
        "def available_optimizers() -> tuple:\n",
        "    \"\"\"\n",
        "    This function is used to provide list of optimizers available for selected neural net architectures.\n",
        "\n",
        "    Parameters:\n",
        "    - (None)\n",
        "\n",
        "    Returns:\n",
        "    - (list,int): Returns tuple contains list of available optimizers and user choice; (available optimizers, user_choice)\n",
        "    \"\"\"\n",
        "\n",
        "    optimizers = ['Adam',\n",
        "                  'AdamW',\n",
        "                  'RMSProp',\n",
        "                  'SGD'] # list of available optimizers\n",
        "\n",
        "    print('Select any one optimizer from the list given below')\n",
        "    for i in range(len(optimizers)):\n",
        "        print(f'{i}_________{optimizers[i]}') # print list of available optimizers with the integer optimizer number\n",
        "\n",
        "    if len(optimizers) == 0:\n",
        "        return optimizers, np.uint(0) # only one optimizer is there; no need to ask to user for their choice\n",
        "    else:\n",
        "        return optimizers, get_user_choice(0, len(optimizers)-1) # get user choice\n",
        "\n",
        "def save_trained_model(model: Any, path:str, model_prefix:str, optimizer:str, epochs:int) -> None:\n",
        "    \"\"\"\n",
        "    This function is used to save trained neural net architecture with .pth extension at specified path.\n",
        "\n",
        "    Parameters:\n",
        "    - model (any): model file which contains metadata with neural network weights\n",
        "    - path (str): path to the directory where model will be saved\n",
        "    - model_prefix (str): model file will be saved with this prefix (ultimately model name)\n",
        "    - optimizer (str): optimizer selected by user\n",
        "    - epochs (int): total number of epochs\n",
        "\n",
        "    Returns:\n",
        "    - (None)\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(path): # check directory exists or not\n",
        "        os.makedirs(path) # if not then create it\n",
        "\n",
        "    model_prefix = model_prefix.replace(' ','_') # replace white space with the underscore if white space is available in the model prefix\n",
        "    model_prefix = model_prefix.replace('(','').replace(')','') # replace any open or close brackets avaialable in model prefix white empty string; removal of brackets\n",
        "\n",
        "    counter = 0 # set counter to zero initially\n",
        "\n",
        "    while True:\n",
        "        model_file_name = f'{model_prefix}_{optimizer}_{str(epochs)}_{counter}.pth' # generate model file name\n",
        "        if not os.path.exists(os.path.join(path, model_file_name)):\n",
        "            break\n",
        "        else:\n",
        "            counter += 1 # increment counter by one\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(path,model_file_name)) # save trained model @ specified path with specified name\n",
        "    print(f'Trained model is successfully saved  at: \\n{os.path.join(path,model_file_name)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW3DD6UiVoI5"
      },
      "source": [
        "### DNASegment Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pQsoifGiVoI6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "class-0: Background class\n",
        "class-1: Peanut Seed\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Which metric to focus on:\n",
        "[1] Dice co-efficient:\n",
        "\n",
        "    -   It measures the overlap between the predicted and groundtruth regions. it is particularly useful\n",
        "        for binary and multi-class segmentation tasks and is sensitive to the presence of small objects.\n",
        "\n",
        "    -   High Dice co-efficient values indicate good segmentation performance.\n",
        "\n",
        "    -   You should focus on the Dice coefficient for each class separately to ensure that the model\n",
        "        performs well across all classes, especially in cases where class imbalance is a concern.\n",
        "\n",
        "[2] mean Intersection over Union (IoU):\n",
        "\n",
        "    -   It measures the average overlap between the predicted and groundtruth regions across all classes.\n",
        "        It is a more stringent (stiff, rigid) metric than the Dice co-efficient because it penalizes\n",
        "        both false positives and false negatives more heavily.\n",
        "\n",
        "    -   High mean Intersection over Union values indicate that the model segments the images accurately\n",
        "        across all classes.\n",
        "\n",
        "    -   You should focus on the mean Intersection over Union to get an overall sense of the model's\n",
        "        segmentation performance across all classes.\n",
        "\n",
        "Class-Specific performance:\n",
        "    -   If you are particularly concerned about the performance on individual classes (e.g. due to\n",
        "        class imbalance or specific importance of certain classes), focus on the Dice co-efficient\n",
        "        for each class. This will help you identify any classes where the model might be\n",
        "        underperforming.\n",
        "\n",
        "    -   If you want an overall measure of segmentation quality that takes into account the performance\n",
        "        across all classes, focus on the mean IoU. This will give you a comprehensive view of how well\n",
        "        the model segments the images on average.\n",
        "\"\"\"\n",
        "\n",
        "# Focal Loss for Dense Object Detection: https://arxiv.org/abs/1708.02002\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self,alpha=1.0,gamma=2.0,reduction='mean') -> None:\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha # assign alpha value\n",
        "        self.gamma = gamma # assign gamma value\n",
        "        self.reduction = reduction # assign reduction value; 'mean', 'none', 'sum'\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        ce_loss = F.cross_entropy(predictions, targets, reduction='none') # cross-entropy loss\n",
        "        p_t = torch.exp(-ce_loss) # probability of the positive class\n",
        "        focal_loss = self.alpha * (1-p_t) ** self.gamma * ce_loss # calculate the focal loss\n",
        "\n",
        "        if self.reduction=='mean': # the losses are averaged over the batch\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction=='sum': # the losses are summed over the batch\n",
        "            return focal_loss.sum()\n",
        "        else: # the loss will return as-is for each element in the batch; no reduction will be applied\n",
        "            return focal_loss\n",
        "\n",
        "# V-Net: Fully CNN for Volumetric Medical Image Segmentation: https://arxiv.org/abs/1606.04797\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1) -> None:\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth # assign smooth param value\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        predictions = torch.sigmoid(predictions) # apply sigmoid to predictions to get probabilities\n",
        "        predictions = predictions.view(-1) # flatten the predictions from (N, H, W) to (N, H*W)\n",
        "        targets = targets.view(-1) # flatten the targets from (N, H, W) to (N, H*W)\n",
        "        intersection = (predictions * targets).sum() # calculate intersections\n",
        "        union = predictions.sum() + targets.sum() # calculate union\n",
        "        dice_coeff = (2. * intersection + self.smooth) / (union + self.smooth) # compute dice co-efficient\n",
        "        dice_loss = 1 - dice_coeff # compute dice loss\n",
        "        return dice_loss\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, beta=0.5) -> None:\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.focal_loss = FocalLoss() # initialize the focal loss\n",
        "        self.dice_loss = BinaryDiceLoss() # initialize the dice loss\n",
        "        self.alpha = alpha # set value of alpha\n",
        "        self.beta = beta # set value of beta\n",
        "        \"\"\"\n",
        "        Adjust alpha and beta based on the performance:\n",
        "        [1] Class imbalance: Increase alpha to give more weight to Focal Loss, which handles\n",
        "            class imbalance well.\n",
        "        [2] Overall Segmentation Accuracy: Increase beta to give more weight to Dice Loss/ Binary Dice Loss\n",
        "            which measures overlap directly.\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        focal_loss = self.focal_loss(predictions,targets) # calculate the focal loss\n",
        "        dice_loss = self.dice_loss(predictions,targets) # calculate the dice loss\n",
        "        combined_loss = self.alpha * focal_loss + self.beta * dice_loss # calculate the linear combination of focal loss and dice loss\n",
        "        return combined_loss # return combined loss\n",
        "\n",
        "# class PatchEmbedding(nn.Module):\n",
        "#     def __init__(self, img_height, img_width, patch_size, in_channels, embed_dim):\n",
        "#         super(PatchEmbedding, self).__init__()\n",
        "#         self.patch_size = patch_size # initialize patch size\n",
        "#         self.num_patches_h = img_height // patch_size # calculate the number of patches based on the image height\n",
        "#         self.num_patches_w = img_width // patch_size # calculate the number of patches based on the image width\n",
        "#         self.num_patches = self.num_patches_h * self.num_patches_w # calculate the total number of patches\n",
        "#         self.embed_dim = embed_dim # initialize embedding dimension\n",
        "\n",
        "#         self.proj = nn.Linear(patch_size * patch_size * in_channels, embed_dim) # linear layer to project each patch to embedding dimension\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.shape # get the batch size, channels, height and width of the input\n",
        "#         x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size) # unfold the input tensor to extract patches\n",
        "#         x = x.contiguous().view(B, C, -1, self.patch_size, self.patch_size) # rearrange the patches to be in the contiguous block of memory\n",
        "#         x = x.permute(0, 2, 3, 4, 1).contiguous().view(B, -1, self.patch_size * self.patch_size * C) # permute the dimension to bring the patch dimension to the front and flatten the patch into single vector\n",
        "#         x = self.proj(x) # project the flattened patches to the embedding dimension\n",
        "#         return x # return the embedded patches\n",
        "\n",
        "# class PatchEmbedding(nn.Module):\n",
        "#     def __init__(self, in_channels=3, patch_size=16, embed_dim=768):\n",
        "#         super().__init__()\n",
        "#         self.patch_size = patch_size\n",
        "#         self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) # change the kernel_size and stride to match the patch_size\n",
        "#         self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.shape # get the batch size, channels, height and width of the input\n",
        "#         # the following line is no longer needed as the conv2d layer automatically extracts patches\n",
        "#         # x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size) # unfold the input tensor to extract patches\n",
        "#         x = self.projection(x) # apply the convolution to extract patches and embed them\n",
        "#         x = x.flatten(2).transpose(1, 2) # flatten the patches and transpose to get the embedding dimension as the last dimension\n",
        "#         x = self.norm(x) # apply layer normalization\n",
        "#         return x\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_height=720, img_width=1280, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.num_patches = (img_height // patch_size) * (img_width // patch_size)  # Calculate num_patches for your image size\n",
        "\n",
        "        self.projection = nn.Conv2d(in_channels=in_channels,\n",
        "                                   out_channels=embed_dim,\n",
        "                                   kernel_size=patch_size,\n",
        "                                   stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, num_patches, embed_dim):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, embed_dim)) # initialize position embeddings as a learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.position_embeddings # add position embeddings to the input tensor X\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads # initialize the number of heads\n",
        "        self.head_dim = embed_dim // num_heads # set dimension of the each attention head\n",
        "        self.scale = self.head_dim ** -0.5 # scaling factor for attention scores\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3) # linear layer to project input to queries, key, and values\n",
        "        self.fc = nn.Linear(embed_dim, embed_dim) # linear layer to project the concatenated outputs of attention heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape # get the batch size, number of patches, and embedding dimension\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim) # project input to queries, keys, and values and reshape for multi-head attention\n",
        "        q, k, v = qkv.permute(2, 0, 3, 1, 4).chunk(3, dim=0) # split the queries, keys, and values for each head\n",
        "\n",
        "        q = q.squeeze(0) # remove the redundant dimension for queries\n",
        "        k = k.squeeze(0) # remove the redundant dimension for keys\n",
        "        v = v.squeeze(0) # remove the redundant dimension for values\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale # compute the attention scores\n",
        "        attn = attn.softmax(dim=-1) # apply softmax to get attention weights\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, D) # compute the output by applying attention weights to the values\n",
        "        out = self.fc(out) # project the concatenated outputs of the attention heads\n",
        "        return out # return the output of the multi-head self-attention\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim) # layer normalization applied before multi-head self-attention\n",
        "        self.norm2 = nn.LayerNorm(embed_dim) # layer normalization applied before the MLP\n",
        "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads) # multi-head self-attention layer\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim), # first linear layer projects from embed_dim to mlp_dim\n",
        "            nn.GELU(), # GELU activation function\n",
        "            nn.Linear(mlp_dim, embed_dim), # second linear layer projects back to embed_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mhsa(self.norm1(x)) # apply layer normalization, then multi-head self-attention, and add the result to the input (Residual Connection)\n",
        "        x = x + self.mlp(self.norm2(x)) # apply layer normalization, then MLP, and add the result to the input (Residual Connection)\n",
        "        return x # return the output\n",
        "\n",
        "class VisionTransformerEncoder(nn.Module):\n",
        "    def __init__(self, img_height, img_width, patch_size, in_channels, embed_dim, num_layers, num_heads, mlp_dim):\n",
        "        super(VisionTransformerEncoder, self).__init__()\n",
        "        self.patch_embedding = PatchEmbedding() # initialize patch embedding\n",
        "        self.positional_encoding = PositionalEncoding(self.patch_embedding.num_patches, embed_dim) # initialize positional embedding\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, mlp_dim) for _ in range(num_layers)\n",
        "        ]) # initialize a list of transformer encoder layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x) # apply patch embedding to input\n",
        "        x = self.positional_encoding(x) # add positional encoding to the embeddings\n",
        "        skip_connections = [] # initialize a list to store skip connections\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x) # apply each transformer encoder layer\n",
        "            skip_connections.append(x) # store the output for skip connections\n",
        "        return x, skip_connections # return the final output and skip connections\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim) # layer normalization before self-attention\n",
        "        self.norm2 = nn.LayerNorm(embed_dim) # layer normalization before cross-attention\n",
        "        self.norm3 = nn.LayerNorm(embed_dim) # layer normalization before MLP\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads) # self-attention layer\n",
        "        self.cross_attn = MultiHeadSelfAttention(embed_dim, num_heads) # cross-attention layer\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim), # first linear layer of MLP\n",
        "            nn.GELU(), # GELU activation function\n",
        "            nn.Linear(mlp_dim, embed_dim), # second linear layer of MLP\n",
        "        )\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        x = x + self.self_attn(self.norm1(x)) # apply self-attention and add residual connection\n",
        "        x = x + self.cross_attn(self.norm2(x), encoder_output) # applt cross-attention and add residual connection\n",
        "        x = x + self.mlp(self.norm3(x)) # apply MLP and add residual connection\n",
        "        return x # return the output of the decoder layer\n",
        "\n",
        "class VisionTransformerDecoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_layers, num_heads, mlp_dim):\n",
        "        super(VisionTransformerDecoder, self).__init__()\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(embed_dim, num_heads, mlp_dim) for _ in range(num_layers)\n",
        "        ]) # initialize a list of transformer decoder layers\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, encoder_output) # apply each transformer decoder layer\n",
        "        return x # return the final output\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2) # initialize the transposed convolution layer for upsampling\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.upconv(x) # apply the transposed convolution to upsample the input\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1) # initialize the first convolution layer with kernel size 3 and padding 1\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1) # initialize the second convolution layer with kernel size 3 and padding 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Shape of x before conv1:\", x.shape)  # Print the shape of x\n",
        "        x = F.relu(self.conv1(x)) # apply the first convolution layer followed by ReLU activation\n",
        "        x = F.relu(self.conv2(x)) # applt the second convolution layer followed by ReLU activation\n",
        "        return x # return the output\n",
        "\n",
        "class DNASegmentModel(pl.LightningModule):\n",
        "    def __init__(self, img_height, img_width, patch_size, in_channels, embed_dim, num_layers, num_heads, mlp_dim, num_classes, optimizer):\n",
        "        super(DNASegmentModel, self).__init__()\n",
        "\n",
        "        self.optimizer = optimizer # set optimizer\n",
        "\n",
        "        # stage-1: encoder, bottleneck, and decoder\n",
        "        self.stage1_encoder = VisionTransformerEncoder(img_height, img_width, patch_size, in_channels, embed_dim, num_layers, num_heads, mlp_dim)\n",
        "        self.stage1_bottleneck = ConvBlock(embed_dim, embed_dim * 2)\n",
        "        self.stage1_decoder = VisionTransformerDecoder(embed_dim, num_layers, num_heads, mlp_dim)\n",
        "\n",
        "        # stage-2: encoder, bottleneck, and decoder\n",
        "        self.stage2_encoder = VisionTransformerEncoder(img_height, img_width, patch_size, in_channels, embed_dim, num_layers, num_heads, mlp_dim)\n",
        "        self.stage2_bottleneck = ConvBlock(embed_dim, embed_dim * 2)\n",
        "        self.stage2_decoder = VisionTransformerDecoder(embed_dim, num_layers, num_heads, mlp_dim)\n",
        "\n",
        "        # upsampling blocks for stage 1\n",
        "        self.upconv_blocks_stage1 = nn.ModuleList([\n",
        "            UpsampleBlock(embed_dim * 2, embed_dim),\n",
        "            UpsampleBlock(embed_dim, embed_dim // 2),\n",
        "            UpsampleBlock(embed_dim // 2, embed_dim // 4),\n",
        "            UpsampleBlock(embed_dim // 4, embed_dim // 8),\n",
        "            UpsampleBlock(embed_dim // 8, embed_dim // 16),\n",
        "            UpsampleBlock(embed_dim // 16, embed_dim // 32),\n",
        "        ])\n",
        "\n",
        "        # upsampling blocks for stage 2\n",
        "        self.upconv_blocks_stage2 = nn.ModuleList([\n",
        "            UpsampleBlock(embed_dim * 2, embed_dim),\n",
        "            UpsampleBlock(embed_dim, embed_dim // 2),\n",
        "            UpsampleBlock(embed_dim // 2, embed_dim // 4),\n",
        "            UpsampleBlock(embed_dim // 4, embed_dim // 8),\n",
        "            UpsampleBlock(embed_dim // 8, embed_dim // 16),\n",
        "            UpsampleBlock(embed_dim // 16, embed_dim // 32),\n",
        "        ])\n",
        "\n",
        "        # final convolution to get the desired number of classes\n",
        "        self.final_conv = nn.Conv2d(embed_dim // 32, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # stage 1\n",
        "        x1, skip_connections1 = self.stage1_encoder(x)\n",
        "        x1 = self.stage1_bottleneck(x1)\n",
        "        x1 = self.stage1_decoder(x1, skip_connections1[-1])\n",
        "\n",
        "        # upsample and concatenate skip connections of stage 1\n",
        "        for i, upconv_block in enumerate(self.upconv_blocks_stage1):\n",
        "            x1 = upconv_block(x1)\n",
        "            if i < len(skip_connections1):\n",
        "                x1 = torch.cat((x1, skip_connections1[-(i+1)]), dim=1)\n",
        "\n",
        "        # concatenate original input image with stage 1 output\n",
        "        x2_input = torch.cat((x, x1), dim=1)\n",
        "\n",
        "        # stage 2\n",
        "        x2, skip_connections2 = self.stage2_encoder(x2_input)\n",
        "        x2 = self.stage2_bottleneck(x2)\n",
        "\n",
        "        # add connections from stage 1 decoder to stage 2 encoder\n",
        "        for i in range(len(skip_connections1)):\n",
        "            x2 = x2 + skip_connections1[i]\n",
        "\n",
        "        x2 = self.stage2_decoder(x2, skip_connections2[-1])\n",
        "\n",
        "        # upsample and concatenate skip connections of stage 2\n",
        "        for i, upconv_block in enumerate(self.upconv_blocks_stage2):\n",
        "            x2 = upconv_block(x2)\n",
        "            if i < len(skip_connections2):\n",
        "                x2 = torch.cat((x2, skip_connections2[-(i+1)]), dim=1)\n",
        "\n",
        "        x2 = self.final_conv(x2)\n",
        "        return x2\n",
        "\n",
        "    def training_step(self, batch, batch_idx) -> None:\n",
        "\n",
        "        images, masks = batch # load input images and input masks from single-single batch\n",
        "        outputs = self(images) # calculate the prediction\n",
        "\n",
        "        # compute metrics\n",
        "        preds = torch.argmax(outputs, dim=1) # convert raw outputs to predicted class labels\n",
        "\n",
        "        combined_loss = CombinedLoss(alpha=0.5, beta=0.5) # initialize the combined loss\n",
        "        loss = combined_loss(outputs,masks) # calculate the focal loss; attention to the below comment about params\n",
        "\n",
        "        \"\"\"\n",
        "        Here in the loss calculation I have passed 'outputs' instead of the 'preds' because\n",
        "        cross-entropy loss function needs the raw logits to compute the probabilities for each\n",
        "        class and then calculate the loss. The cross-entropy function internally applies the\n",
        "        softmax operation to the logits to compute the probabilities and cross-entropy loss.\n",
        "        \"\"\"\n",
        "\n",
        "        mean_iou_score = self.mean_iou(preds,masks) # calculate the mean iou score over all the classes\n",
        "        \"\"\"\n",
        "        Here in the mean_iou function, \"preds\" parameter is passed because here there is need of\n",
        "        predicted label classes, not need of the raw logits.\n",
        "        \"\"\"\n",
        "\n",
        "        dice_coeff_bg, dice_coeff_peanut_seed = self.dice_coefficient(preds,masks) # calculate the dice_coefficient separately for all avaiilable classes\n",
        "\n",
        "        # log metrics\n",
        "        self.log('DNASegment_train_combined_loss',loss,on_step=True,on_epoch=True,prog_bar=True,enable_graph=True) # save the loss logs for visualization\n",
        "        self.log('DNASegment_train_mean_IoU',mean_iou_score,on_step=True,on_epoch=True,prog_bar=True,enable_graph=True) # save the mean IoU logs for visualization\n",
        "        self.log('DNASegment_train_dice_coeff_bg',dice_coeff_bg,on_step=True,on_epoch=True,prog_bar=True,enable_graph=True) # save the dice_coeff_bg logs for visualization\n",
        "        self.log('DNASegment_train_dice_coeff_peanut_seed',dice_coeff_peanut_seed,on_step=True,on_epoch=True,prog_bar=True,enable_graph=True) # save the dice_coeff_peanut_seed logs for visualization\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx) -> None:\n",
        "\n",
        "        images, masks = batch # load input images and input masks from single-single batch\n",
        "        outputs = self(images) # calculate the prediction\n",
        "\n",
        "        # compute metrics\n",
        "        preds = torch.argmax(outputs, dim=1) # convert raw outputs to predicted class labels\n",
        "\n",
        "        combined_loss = CombinedLoss(alpha=0.5, beta=0.5) # initialize the combined loss\n",
        "        loss = combined_loss(outputs,masks) # calculate the focal loss; attention to the below comment about params\n",
        "\n",
        "        \"\"\"\n",
        "        Here in the loss calculation I have passed 'outputs' instead of the 'preds' because\n",
        "        cross-entropy loss function needs the raw logits to compute the probabilities for each\n",
        "        class and then calculate the loss. The cross-entropy function internally applies the\n",
        "        softmax operation to the logits to compute the probabilities and cross-entropy loss.\n",
        "        \"\"\"\n",
        "\n",
        "        mean_iou_score = self.mean_iou(preds,masks) # calculate the mean iou score over all the classes\n",
        "        \"\"\"\n",
        "        Here in the mean_iou function, \"preds\" parameter is passed because here there is need of\n",
        "        predicted label classes, not need of the raw logits.\n",
        "        \"\"\"\n",
        "\n",
        "        dice_coeff_bg, dice_coeff_peanut_seed = self.dice_coefficient(preds,masks) # calculate the dice_coefficient separately for all avaiilable classes\n",
        "\n",
        "        # log metrics\n",
        "        self.log('DNASegment_validation_combined_loss',loss,on_step=True,on_epoch=True,prog_bar=True,enable_graph=True) # save the loss logs for visualization\n",
        "        self.log('DNASegment_validation_mean_IoU',mean_iou_score,on_step=True,on_epoch=True,prog_bar=True,enable_graph=True) # save the mean IoU logs for visualization\n",
        "        self.log('DNASegment_validation_dice_coeff_bg',dice_coeff_bg,on_step=True,on_epoch=True,prog_bar=True,enable_graph=True) # save the dice_coeff_bg logs for visualization\n",
        "        self.log('DNASegment_validation_dice_coeff_peanut_seed',dice_coeff_peanut_seed,on_step=True,on_epoch=True,prog_bar=True,enable_graph=True) # save the dice_coeff_peanut_seed logs for visualization\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def mean_iou(self, predictions, targets, num_classes=2) -> Any:\n",
        "        ious = [] # create an empty list to store class-wise ious\n",
        "        predictions = predictions.view(-1) # flatten the predictions from (N, H, W) to (N, H*W)\n",
        "        targets = targets.view(-1) # flatten the targets from (N, H, W) to (N, H*W)\n",
        "\n",
        "        for cls in range(num_classes): # iterate over each class\n",
        "            predictions_inds = predictions == cls # create a binary mask for the current class in the predictions\n",
        "            target_inds = targets == cls # create a binary mask for the current class in the targets\n",
        "            intersection = (predictions_inds & target_inds).sum().item() # calculate the intersection for the current class\n",
        "            union = (predictions_inds | target_inds).sum().item() # calculate the union for the current class\n",
        "\n",
        "            if union == 0:\n",
        "                ious.append(float('nan')) # if there is no groundtruth, do not include this in IoU calculation\n",
        "            else:\n",
        "                iou = intersection / union # calculate the IoU for the current class\n",
        "                ious.append(iou) # append the IoU of the cuurent class to the list\n",
        "\n",
        "        mean_iou = torch.tensor(ious).mean() # return the mean iou over all classes\n",
        "\n",
        "        return mean_iou.item() # return a python scalar instead of torch tensor\n",
        "\n",
        "    def dice_coefficient(self, predictions, targets, num_classes=2, smooth=1e-5) -> Any:\n",
        "        dice_scores = [] # define empty list to store class-wise dice scores\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            predictions = (predictions == cls).float().view(-1) # creates binary mask where pixels belonging to the current class 'cls' are marked as 1 and others as 0\n",
        "            targets = (targets == cls).float().view(-1) # create binary mask\n",
        "            intersection = (predictions * targets).sum() # calculate the intersection for the current class\n",
        "            union = predictions.sum() + targets.sum() # calculate the union for the current class\n",
        "            dice_coeff = (2. * intersection + smooth)/(union + smooth) # calculate the dice_coeff for the current class\n",
        "            dice_scores.append(dice_coeff.item()) # convert tensor to scalar and append to a list\n",
        "\n",
        "        return tuple(dice_scores) # return the dice co-efficient as tuple\n",
        "    \"\"\"\n",
        "    [1] LLRD: Layer-wise Learning Rate Decay\n",
        "    [2] Weight Decay: L2-Regularization\n",
        "    [3] Drop Path Rate (Stochastic Path): Randomly drops entire layers during training to help prevent\n",
        "            overfitting and improve the robustness of the model.\n",
        "    \"\"\"\n",
        "    def configure_optimizers(self):\n",
        "        if self.optimizer.lower() == 'adam':\n",
        "            return torch.optim.Adam(self.parameters(),\n",
        "                                    lr=0.0001,\n",
        "                                    betas=(0.9,0.999),\n",
        "                                    weight_decay=0.1) # set adam optimizer with 0.0001 learning rate\n",
        "        elif self.optimizer.lower() == 'adamw':\n",
        "            return torch.optim.AdamW(self.parameters(),\n",
        "                                     lr=0.0001,\n",
        "                                     betas=(0.9,0.999),\n",
        "                                     weight_decay=0.1) # set adamw optimizer with 0.0001 learning rate\n",
        "        elif self.optimizer.lower() == 'rmsprop':\n",
        "            return torch.optim.RMSprop(self.parameters(),\n",
        "                                       lr=0.0001,\n",
        "                                       weight_decay=0.1) # set RMSProp optimizer with 0.0001 learning rate\n",
        "        elif self.optimizer.lower() == 'sgd':\n",
        "            return torch.optim.SGD(self.parameters(),\n",
        "                                   lr=0.0001,\n",
        "                                   weight_decay=0.1) # set SGD optimizer with 0.0001 learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLOvwew9VoI7"
      },
      "source": [
        "### Train DNASegment Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f53f1215f0a9435ea02795ee0731e284",
            "d4e7c348c58b41bdbaaae3caea0ca756",
            "36e4afda8bc94be9b7d6110369454027",
            "f075106bb1fc49ceaaab582946f122b6",
            "7ba78038e351400fbaa7a1be07981d0f",
            "8d897d5547034c6f96c11335fdbdd973",
            "b82e02c095094cdfabe4289fffc75e42",
            "508a87ce4dfb44c0b3407e18973a1b50",
            "98b4448d239b4dfb8199934ba5724e95",
            "677057c46d6f40b8a68286c58d59c5f2",
            "af212f15dc834ae8b4433e837c9af476"
          ]
        },
        "id": "jkoEuHmFVoI8",
        "outputId": "35b7693e-703e-4a74-ab20-e763bf04bbdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Number of GPUs available: 1\n",
            "- GPU name: Tesla T4\n",
            "Wed Jul 31 17:27:20 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0              30W /  70W |   4985MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "\n",
            "-----------------------------------------------------------\n",
            "-----------------------CONFIGURATIONS----------------------\n",
            "-----------------------------------------------------------\n",
            "\n",
            " - Current working directory: /content/drive/MyDrive/Notebook_Testing/\n",
            " - Trainset path: \n",
            " - Validset path: \n",
            " - Train image directory: \n",
            " - Train mask directory: \n",
            " - Input JSON file path: /content/drive/MyDrive/Notebook_Testing/data/raw/json_projects\n",
            " - Sample JSON file path: /content/drive/MyDrive/Notebook_Testing/data/raw/json_projects/File1.json\n",
            " - Raw image directory: /content/drive/MyDrive/Notebook_Testing/data/raw/images\n",
            " - Base data path: /content/drive/MyDrive/Notebook_Testing/data\n",
            " - Path to save trained model: /content/drive/MyDrive/Notebook_Testing/saved_models\n",
            " - Batch size: 16\n",
            " - Maximum epochs: 2\n",
            " - Number of classes: 2\n",
            " - Learning rate: 0.001\n",
            " - Tranformation/Augmentation: True\n",
            "\n",
            "-----------------------------------------------------------\n",
            "--------------UNDERSTANDING JSON FILE STRUCTURE------------\n",
            "-----------------------------------------------------------\n",
            "- Keys available in json data: \n",
            "dict_keys(['_via_settings', '_via_img_metadata', '_via_attributes', '_via_data_format_version', '_via_image_id_list'])\n",
            "\n",
            "- Keys in value of '_via_img_metadata' key: \n",
            "dict_keys(['9.jpeg136245', '32.jpeg84808', '31.jpeg75062', '33.jpeg75585', '34.jpeg77045', '36.jpeg63248', '35.jpeg78179', '37.jpeg65506', '38.jpeg105805', '39.jpeg133898', '40.jpeg132537', '0.jpeg98071', '1.jpeg142483', '2.jpeg133898', '3.jpeg129789', '4.jpeg132537', '5.jpeg130796', '6.jpeg128702', '7.jpeg130190', '8.jpeg132517', '10.jpeg135565', '11.jpeg111857', '12.jpeg112995', '13.jpeg114321', '14.jpeg118938', '15.jpeg118605', '16.jpeg121895', '17.jpeg126332', '19.jpeg105219', '18.jpeg124046'])\n",
            "\n",
            "- Keys in value of '9.jpeg136245' key: \n",
            "dict_keys(['filename', 'size', 'regions', 'file_attributes'])\n",
            "\n",
            "- Filename: 9.jpeg\n",
            "- Regions: 181\n",
            "- First region information:\n",
            "-- Class name: peanut_seed\n",
            "-- X co-ordinates: \n",
            "[167, 183, 192, 208, 220, 226, 229, 232, 230, 227, 225, 225, 225, 221, 214, 207, 198, 185, 175, 167, 161, 155, 151, 151, 157]\n",
            "-- Y co-ordinates: \n",
            "[93, 85, 86, 94, 105, 114, 120, 140, 146, 145, 146, 150, 154, 161, 172, 175, 175, 173, 170, 165, 159, 148, 132, 119, 103]\n",
            "\n",
            "-----------------------------------------------------------\n",
            "------------------CREATING PROCESSED DATASET---------------\n",
            "-----------------------------------------------------------\n",
            "Merging 2 json files.\n",
            "2 json files are merged successfully.\n",
            "Creating mask images from json data...\n",
            "File has been removed from processed dataset.: /content/drive/MyDrive/Notebook_Testing/data/processed/train/images/32.jpeg\n",
            "File is not available in the dataset.: /content/drive/MyDrive/Notebook_Testing/data/processed/train/images/32.jpeg\n",
            "Mask image are created successfully.\n",
            "Do you want to visualize mask-image pairs? [Y/N]: n\n",
            "-----------------------------------------------------------\n",
            "---------------NN ARCHITECTURE (MODEL) SELECTION-----------\n",
            "-----------------------------------------------------------\n",
            "Select any one neural net architecture from the list given below\n",
            "0_________DNA-Segment\n",
            "Select any one optimizer from the list given below\n",
            "0_________Adam\n",
            "1_________AdamW\n",
            "2_________RMSProp\n",
            "3_________SGD\n",
            "Enter an integer number between 0 and 3: 0\n",
            "DNA-Segment neural net architecture is selected with Adam optimizer.\n",
            "- Model summary:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                 | Type                     | Params | Mode \n",
            "--------------------------------------------------------------------------\n",
            "0 | stage1_encoder       | VisionTransformerEncoder | 130 M  | train\n",
            "1 | stage1_bottleneck    | ConvBlock                | 31.9 M | train\n",
            "2 | stage1_decoder       | VisionTransformerDecoder | 170 M  | train\n",
            "3 | stage2_encoder       | VisionTransformerEncoder | 130 M  | train\n",
            "4 | stage2_bottleneck    | ConvBlock                | 31.9 M | train\n",
            "5 | stage2_decoder       | VisionTransformerDecoder | 170 M  | train\n",
            "6 | upconv_blocks_stage1 | ModuleList               | 6.3 M  | train\n",
            "7 | upconv_blocks_stage2 | ModuleList               | 6.3 M  | train\n",
            "8 | final_conv           | Conv2d                   | 50     | train\n",
            "--------------------------------------------------------------------------\n",
            "678 M     Trainable params\n",
            "0         Non-trainable params\n",
            "678 M     Total params\n",
            "2,713.713 Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------\n",
            "---------------NN ARCHITECTURE (MODEL) TRAINING------------\n",
            "-----------------------------------------------------------\n",
            "Training started...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f53f1215f0a9435ea02795ee0731e284",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 4.06 GiB. GPU ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-34afe6d4437a>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training started...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train the neural network architecture selected by user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training finished.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         )\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         )\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-faf921b24763>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;31m# load input images and input masks from single-single batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculate the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-faf921b24763>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# stage 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_connections1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_bottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_connections1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-faf921b24763>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mskip_connections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# initialize a list to store skip connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply each transformer encoder layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0mskip_connections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# store the output for skip connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_connections\u001b[0m \u001b[0;31m# return the final output and skip connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-faf921b24763>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmhsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply layer normalization, then multi-head self-attention, and add the result to the input (Residual Connection)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply layer normalization, then MLP, and add the result to the input (Residual Connection)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;31m# return the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-faf921b24763>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# remove the redundant dimension for values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;31m# compute the attention scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply softmax to get attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.06 GiB. GPU "
          ]
        }
      ],
      "source": [
        "check_gpu_config() # get GPU (General Processing Unit) information if it is available\n",
        "\n",
        "config = Config() # create an instance of Config class\n",
        "config.printConfiguration() # print all configuration set by defualt\n",
        "\n",
        "# wandb.init(entity=config.ENTITY, # assign team/organization name\n",
        "#                project=config.PROJECT, # assign project name\n",
        "#                anonymous=config.ANONYMOUS, # set anonymous value type\n",
        "#                reinit=config.REINIT) # initialize the weights and biases cloud server instance\n",
        "\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(\"--------------UNDERSTANDING JSON FILE STRUCTURE------------\")\n",
        "print(\"-----------------------------------------------------------\")\n",
        "\n",
        "look_at_json_structure(config.SAMPLE_JSON_FILE_PATH) # understand JSON file structure and which information it contains\n",
        "\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(\"------------------CREATING PROCESSED DATASET---------------\")\n",
        "print(\"-----------------------------------------------------------\")\n",
        "\n",
        "input_files = glob.glob(f'{config.INPUT_JSON_FILE_PATH}/File*.json') # create list of json files available @ config.INPUT_JSON_FILE_PATH\n",
        "output_file = f'{config.INPUT_JSON_FILE_PATH}/Merge.json' # define path for json file, contains merged data from multiple json files\n",
        "merge_json_files(input_files=input_files, output_file=output_file) # merge multiple json files\n",
        "\n",
        "print(\"Creating mask images from json data...\")\n",
        "createMasks(json_file_path=output_file,\n",
        "                raw_image_dir=config.RAW_IMAGE_DIR,\n",
        "                base_data_path=config.BASE_DATA_PATH) # create mask images from existing mask region information i.e. XY co-ordinates\n",
        "\n",
        "config.TRAINSET_PATH = os.path.join(config.BASE_DATA_PATH,f'processed/train')\n",
        "\n",
        "show_mask_image_pair(image_dir=os.path.join(config.TRAINSET_PATH,'images'),\n",
        "                         mask_dir=os.path.join(config.TRAINSET_PATH,'masks')) # visualize mask-image pairs\n",
        "\n",
        "config.TRAIN_IMAGE_DIR = os.path.join(config.TRAINSET_PATH,'images') # set path to a directory contains input images for training\n",
        "config.TRAIN_MASK_DIR = os.path.join(config.TRAINSET_PATH, 'masks') # set path to a directory contains input mask images for training\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # set device for model training\n",
        "data_module = SegmentationDataModule(train_image_dir=config.TRAIN_IMAGE_DIR,\n",
        "                                         train_mask_dir=config.TRAIN_MASK_DIR,\n",
        "                                         batch_size=config.BATCH_SIZE,\n",
        "                                         transform=config.TRANSFORM) # initialize the data module\n",
        "\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(\"---------------NN ARCHITECTURE (MODEL) SELECTION-----------\")\n",
        "print(\"-----------------------------------------------------------\")\n",
        "\n",
        "available_nn_archs, user_choice_nn_arch = available_models() # give list of available neural net architectures to user for training\n",
        "available_optims, user_choice_optimizer = available_optimizers() # give list of available optimizers to user for neural net configuration\n",
        "print(f'{available_nn_archs[user_choice_nn_arch]} neural net architecture is selected with {available_optims[user_choice_optimizer]} optimizer.')\n",
        "\n",
        "if user_choice_nn_arch == 0:\n",
        "        model = DNASegmentModel(\n",
        "            img_height=720, # image height\n",
        "            img_width=1280, # image width\n",
        "            patch_size=16, # single patch size\n",
        "            in_channels=3, # number of channels in the input images\n",
        "            embed_dim=768, # embedding dimension\n",
        "            num_layers=18, # number of layers in the transformers\n",
        "            num_heads=12, # number of heads\n",
        "            mlp_dim=3072, # MLP dimension\n",
        "            num_classes=2, # total number of output classes\n",
        "            optimizer=available_optims[user_choice_optimizer]) # set optimizer\n",
        "\n",
        "print('- Model summary:\\n')\n",
        "# summary(model=model,\n",
        "#             input_size=(1,3,720,1280),\n",
        "#             col_names=['input_size','output_size','kernel_size']) # print model summary; input shape is extracted @ data loading time\n",
        "\n",
        "model = model.to(device) # move neural net architecture to available computing device\n",
        "# wandb_logger = WandbLogger(log_model=config.LOG_MODEL) # initialize the weights-and-biases logger\n",
        "\n",
        "# trainer = pl.Trainer(max_epochs=config.MAX_EPOCHS, # set maximum number of epochs\n",
        "#                          log_every_n_steps=1, # after how many 'n' steps log will be saved\n",
        "#                          logger=wandb_logger) # assign logger for saving a logs\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=config.MAX_EPOCHS, # set maximum number of epochs\n",
        "                         log_every_n_steps=1) # assign logger for saving a logs\n",
        "\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(\"---------------NN ARCHITECTURE (MODEL) TRAINING------------\")\n",
        "print(\"-----------------------------------------------------------\")\n",
        "\n",
        "print('Training started...')\n",
        "trainer.fit(model, data_module) # train the neural network architecture selected by user\n",
        "print('Training finished.')\n",
        "\n",
        "# wandb.finish() # close the weights and biases cloud instance\n",
        "\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(\"----------------------SAVE TRAINED MODEL-------------------\")\n",
        "print(\"-----------------------------------------------------------\")\n",
        "\n",
        "print('Saving trained model..')\n",
        "save_trained_model(model=model, # model\n",
        "                       path=config.PATH_TO_SAVE_TRAINED_MODEL, # path to save trained model\n",
        "                       model_prefix=available_nn_archs[user_choice_nn_arch], # model name\n",
        "                       optimizer=available_optims[user_choice_optimizer], # selected optimizer and max. epochs\n",
        "                       epochs=config.MAX_EPOCHS) # save trained neural network architecture in the .pth format\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36e4afda8bc94be9b7d6110369454027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_508a87ce4dfb44c0b3407e18973a1b50",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98b4448d239b4dfb8199934ba5724e95",
            "value": 0
          }
        },
        "508a87ce4dfb44c0b3407e18973a1b50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "677057c46d6f40b8a68286c58d59c5f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ba78038e351400fbaa7a1be07981d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8d897d5547034c6f96c11335fdbdd973": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98b4448d239b4dfb8199934ba5724e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af212f15dc834ae8b4433e837c9af476": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b82e02c095094cdfabe4289fffc75e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4e7c348c58b41bdbaaae3caea0ca756": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d897d5547034c6f96c11335fdbdd973",
            "placeholder": "​",
            "style": "IPY_MODEL_b82e02c095094cdfabe4289fffc75e42",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "f075106bb1fc49ceaaab582946f122b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_677057c46d6f40b8a68286c58d59c5f2",
            "placeholder": "​",
            "style": "IPY_MODEL_af212f15dc834ae8b4433e837c9af476",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "f53f1215f0a9435ea02795ee0731e284": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4e7c348c58b41bdbaaae3caea0ca756",
              "IPY_MODEL_36e4afda8bc94be9b7d6110369454027",
              "IPY_MODEL_f075106bb1fc49ceaaab582946f122b6"
            ],
            "layout": "IPY_MODEL_7ba78038e351400fbaa7a1be07981d0f"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
